{
  "model_id": "qwen3-next-80b-a3b-thinking",
  "name": "Qwen3-Next-80B-A3B-Thinking",
  "organization_id": "qwen",
  "model_family_id": null,
  "fine_tuned_from_model_id": null,
  "description": "Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks â€” outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",
  "release_date": "2025-01-10",
  "announcement_date": "2025-01-10",
  "license_id": "apache_2_0",
  "multimodal": false,
  "knowledge_cutoff": null,
  "param_count": 80000000000,
  "training_tokens": 15000000000000,
  "available_in_zeroeval": true,
  "source_api_ref": "https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking",
  "source_playground": "https://chat.qwen.ai/",
  "source_paper": null,
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen3-next/",
  "source_repo_link": "https://github.com/QwenLM/Qwen3",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking",
  "created_at": "2025-01-10T00:00:00.000000+00:00",
  "updated_at": "2025-09-15T00:00:00.000000+00:00"
}

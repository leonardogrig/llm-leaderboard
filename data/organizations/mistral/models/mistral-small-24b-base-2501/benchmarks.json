[
  {
    "model_benchmark_id": 1411,
    "benchmark_id": "agieval",
    "model_id": "mistral-small-24b-base-2501",
    "score": 0.658,
    "normalized_score": 0.658,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:13.980585+00:00",
    "updated_at": "2025-07-19T19:56:13.980585+00:00",
    "benchmark_name": "AGIEval"
  },
  {
    "model_benchmark_id": 31,
    "benchmark_id": "arc-c",
    "model_id": "mistral-small-24b-base-2501",
    "score": 0.9129,
    "normalized_score": 0.9129,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501",
    "verified_by_llmstats": false,
    "analysis_method": "0-shot",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.143960+00:00",
    "updated_at": "2025-07-19T19:56:11.143960+00:00",
    "benchmark_name": "ARC-C"
  },
  {
    "model_benchmark_id": 345,
    "benchmark_id": "gpqa",
    "model_id": "mistral-small-24b-base-2501",
    "score": 0.3437,
    "normalized_score": 0.3437,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501",
    "verified_by_llmstats": false,
    "analysis_method": "5-shot, CoT",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.748111+00:00",
    "updated_at": "2025-07-19T19:56:11.748111+00:00",
    "benchmark_name": "GPQA"
  },
  {
    "model_benchmark_id": 1013,
    "benchmark_id": "gsm8k",
    "model_id": "mistral-small-24b-base-2501",
    "score": 0.8073,
    "normalized_score": 0.8073,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501",
    "verified_by_llmstats": false,
    "analysis_method": "5-shot, maj@1",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:13.111924+00:00",
    "updated_at": "2025-07-19T19:56:13.111924+00:00",
    "benchmark_name": "GSM8k"
  },
  {
    "model_benchmark_id": 424,
    "benchmark_id": "math",
    "model_id": "mistral-small-24b-base-2501",
    "score": 0.4598,
    "normalized_score": 0.4598,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501",
    "verified_by_llmstats": false,
    "analysis_method": "5-shot, MaJ",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.898806+00:00",
    "updated_at": "2025-07-19T19:56:11.898806+00:00",
    "benchmark_name": "MATH"
  },
  {
    "model_benchmark_id": 1195,
    "benchmark_id": "mbpp",
    "model_id": "mistral-small-24b-base-2501",
    "score": 0.6964,
    "normalized_score": 0.6964,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501",
    "verified_by_llmstats": false,
    "analysis_method": "Pass@1",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:13.516399+00:00",
    "updated_at": "2025-07-19T19:56:13.516399+00:00",
    "benchmark_name": "MBPP"
  },
  {
    "model_benchmark_id": 113,
    "benchmark_id": "mmlu",
    "model_id": "mistral-small-24b-base-2501",
    "score": 0.8073,
    "normalized_score": 0.8073,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501",
    "verified_by_llmstats": false,
    "analysis_method": "5-shot",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.311218+00:00",
    "updated_at": "2025-07-19T19:56:11.311218+00:00",
    "benchmark_name": "MMLU"
  },
  {
    "model_benchmark_id": 217,
    "benchmark_id": "mmlu-pro",
    "model_id": "mistral-small-24b-base-2501",
    "score": 0.5437,
    "normalized_score": 0.5437,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501",
    "verified_by_llmstats": false,
    "analysis_method": "0-shot CoT",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.511957+00:00",
    "updated_at": "2025-07-19T19:56:11.511957+00:00",
    "benchmark_name": "MMLU-Pro"
  },
  {
    "model_benchmark_id": 254,
    "benchmark_id": "triviaqa",
    "model_id": "mistral-small-24b-base-2501",
    "score": 0.8032,
    "normalized_score": 0.8032,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501",
    "verified_by_llmstats": false,
    "analysis_method": "5-shot",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.585944+00:00",
    "updated_at": "2025-07-19T19:56:11.585944+00:00",
    "benchmark_name": "TriviaQA"
  }
]
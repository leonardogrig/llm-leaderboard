[
  {
    "model_benchmark_id": 1465,
    "benchmark_id": "arena-hard",
    "model_id": "mistral-small-24b-instruct-2501",
    "score": 0.876,
    "normalized_score": 0.876,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",
    "verified_by_llmstats": false,
    "analysis_method": "Score",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:14.120697+00:00",
    "updated_at": "2025-07-19T19:56:14.120697+00:00",
    "benchmark_name": "Arena Hard"
  },
  {
    "model_benchmark_id": 344,
    "benchmark_id": "gpqa",
    "model_id": "mistral-small-24b-instruct-2501",
    "score": 0.453,
    "normalized_score": 0.453,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",
    "verified_by_llmstats": false,
    "analysis_method": "5 shot COT",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.746578+00:00",
    "updated_at": "2025-07-19T19:56:11.746578+00:00",
    "benchmark_name": "GPQA"
  },
  {
    "model_benchmark_id": 807,
    "benchmark_id": "humaneval",
    "model_id": "mistral-small-24b-instruct-2501",
    "score": 0.848,
    "normalized_score": 0.848,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",
    "verified_by_llmstats": false,
    "analysis_method": "5 shot COT",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:12.682647+00:00",
    "updated_at": "2025-07-19T19:56:12.682647+00:00",
    "benchmark_name": "HumanEval"
  },
  {
    "model_benchmark_id": 630,
    "benchmark_id": "ifeval",
    "model_id": "mistral-small-24b-instruct-2501",
    "score": 0.829,
    "normalized_score": 0.829,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",
    "verified_by_llmstats": false,
    "analysis_method": "Score",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:12.295754+00:00",
    "updated_at": "2025-07-19T19:56:12.295754+00:00",
    "benchmark_name": "IFEval"
  },
  {
    "model_benchmark_id": 423,
    "benchmark_id": "math",
    "model_id": "mistral-small-24b-instruct-2501",
    "score": 0.706,
    "normalized_score": 0.706,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",
    "verified_by_llmstats": false,
    "analysis_method": "instruct",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.896887+00:00",
    "updated_at": "2025-07-19T19:56:11.896887+00:00",
    "benchmark_name": "MATH"
  },
  {
    "model_benchmark_id": 216,
    "benchmark_id": "mmlu-pro",
    "model_id": "mistral-small-24b-instruct-2501",
    "score": 0.663,
    "normalized_score": 0.663,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",
    "verified_by_llmstats": false,
    "analysis_method": "5 shot COT",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.510254+00:00",
    "updated_at": "2025-07-19T19:56:11.510254+00:00",
    "benchmark_name": "MMLU-Pro"
  },
  {
    "model_benchmark_id": 1613,
    "benchmark_id": "mt-bench",
    "model_id": "mistral-small-24b-instruct-2501",
    "score": 0.835,
    "normalized_score": 0.835,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",
    "verified_by_llmstats": false,
    "analysis_method": "Score",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:14.537073+00:00",
    "updated_at": "2025-07-19T19:56:14.537073+00:00",
    "benchmark_name": "MT-Bench"
  },
  {
    "model_benchmark_id": 1818,
    "benchmark_id": "wild-bench",
    "model_id": "mistral-small-24b-instruct-2501",
    "score": 0.522,
    "normalized_score": 0.522,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",
    "verified_by_llmstats": false,
    "analysis_method": "Score",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:15.128734+00:00",
    "updated_at": "2025-07-19T19:56:15.128734+00:00",
    "benchmark_name": "Wild Bench"
  }
]